{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: XGBoost\n",
    "\n",
    "For the part 3, we select XGBoost to implement a learning procedure.\n",
    "\n",
    "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(\"xgboost\")\n",
    "install.packages(\"Matrix\")\n",
    "install.packages(\"MatrixModels\")\n",
    "install.packages(\"data.table\")\n",
    "\n",
    "library(xgboost)\n",
    "library(Matrix)\n",
    "library(MatrixModels)\n",
    "library(data.table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgb <- data\n",
    "train_xgb <- subset(train_xgb, select = c(-id,-status_group))\n",
    "train_xgb <- as.matrix(as.data.frame(lapply(train_xgb, as.numeric)))\n",
    "label_xgb <- data$status_group\n",
    "label_xgb <-as.numeric(label_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb.DMatrix\n",
    "\n",
    "XGBoost offers a way to group meta-data in a xgb.DMatrix.  This will be useful for the most advanced features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.DMatrix <- xgb.DMatrix(data = train_xgb,label = label_xgb, missing = NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multi:softmax: set XGBoost to do multiclass classification using the softmax objective.\n",
    "\n",
    "num_class: number of classes. \n",
    "\n",
    "nrounds:  the max number of iterations. \n",
    "\n",
    "nfold: The dataset is randomly partitioned into nfold equal size subsamples. \n",
    "\n",
    "early_stopping_rounds: integer, means that training with a validation set will stop if the performance doesn't improve for k rounds. \n",
    "\n",
    "booster: which booster to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.tab = xgb.cv(data = train.DMatrix, objective = \"multi:softmax\", booster = \"gbtree\",\n",
    "                 nrounds = 500, nfold = 4, early_stopping_rounds = 10, num_class = 4, maximize = FALSE,\n",
    "                 evaluation = \"merror\", eta = .2, max_depth = 12, colsample_bytree = .4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost has several features to help view the learning progress internally. The purpose is to help to set the best parameters, which is the key of your model quality. One of the simplest way to see the training progress is to set the verbose option. \n",
    "\n",
    "verbose = 1, print evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 <- xgboost(data = train.DMatrix, objective = \"multi:softmax\", booster = \"gbtree\",\n",
    "                  eval_metric = \"merror\", nrounds = 33, \n",
    "                  num_class = 4,eta = .2, max_depth = 14, colsample_bytree = .4,  verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing <- test[ , -which(names(test) %in% c(\"id\"))]\n",
    "test_xgb <- as.matrix(as.data.frame(lapply(testing, as.numeric)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict4 <- predict(model4, test_xgb)\n",
    "predict4[predict4==1]<-\"functional\"\n",
    "predict4[predict4==2]<-\"functional needs repair\"\n",
    "predict4[predict4==3]<-\"non functional\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(predict4)\n",
    "predict4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to check if there are highly correlated features in the dataset. There are several types of importance in the Xgboost - it can be computed in several different ways. In the projcet the gain type is selected to compute the importance. The gain type shows the average gain across all splits where feature was used.\n",
    "\n",
    "It could be found that the features date_recorded and quantity_group have a high importance value for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_importance <- xgb.importance(feature_names = colnames(train_xgb), model =model4)\n",
    "xgb.plot.importance(importance_matrix = xgb_importance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
